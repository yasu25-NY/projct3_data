{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0e88e3-1094-4fdd-a5d7-cf0d3a0b0f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.93.0)\n",
      "Requirement already satisfied: tiktoken in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: pymupdf in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gryphony2/.pyenv/versions/3.12.6/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai tiktoken pymupdf pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7459fdb2-bfd2-4363-85f0-ae39b119ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08e5d102-6ef9-4cfb-b7e5-e092ff6b2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クライアント初期化\n",
    "client = OpenAI(api_key=\"apiキー\")\n",
    "\n",
    "# List of PDF files\n",
    "pdf_paths = {\n",
    "    \"US\": \"Americas-AI-Action-Plan.pdf\",\n",
    "    \"Japan\": \"japan_AI_stratagy2022.pdf\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df336d71-e575-4a5b-bee1-4fadcd746c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: PDF読み込み＆テキスト抽出（PyMuPDF）\n",
    "def load_pdfs(paths: dict) -> dict:\n",
    "    texts = {}\n",
    "    for country, path in paths.items():\n",
    "        print(f\"[load_pdfs] Loading {country} from {path}\")               # ← デバッグ出力\n",
    "        doc = fitz.open(path)\n",
    "        full_text = \"\"\n",
    "        for i, page in enumerate(doc):\n",
    "            page_text = page.get_text()\n",
    "            print(f\"[load_pdfs]  Page {i+1}: {len(page_text)} chars\")    # ← 各ページ文字数\n",
    "            full_text += page_text\n",
    "        print(f\"[load_pdfs]  Total {country}: {len(full_text)} chars\\n\")  # ← 全文文字数\n",
    "        texts[country] = full_text\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0128bfa-17a8-42c4-95ba-a3c6ff8f1ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_pdfs] Loading US from Americas-AI-Action-Plan.pdf\n",
      "[load_pdfs]  Page 1: 88 chars\n",
      "[load_pdfs]  Page 2: 690 chars\n",
      "[load_pdfs]  Page 3: 4496 chars\n",
      "[load_pdfs]  Page 4: 3622 chars\n",
      "[load_pdfs]  Page 5: 1385 chars\n",
      "[load_pdfs]  Page 6: 3042 chars\n",
      "[load_pdfs]  Page 7: 3067 chars\n",
      "[load_pdfs]  Page 8: 3118 chars\n",
      "[load_pdfs]  Page 9: 3527 chars\n",
      "[load_pdfs]  Page 10: 3239 chars\n",
      "[load_pdfs]  Page 11: 3044 chars\n",
      "[load_pdfs]  Page 12: 2806 chars\n",
      "[load_pdfs]  Page 13: 2512 chars\n",
      "[load_pdfs]  Page 14: 3252 chars\n",
      "[load_pdfs]  Page 15: 2928 chars\n",
      "[load_pdfs]  Page 16: 1132 chars\n",
      "[load_pdfs]  Page 17: 3357 chars\n",
      "[load_pdfs]  Page 18: 3408 chars\n",
      "[load_pdfs]  Page 19: 2767 chars\n",
      "[load_pdfs]  Page 20: 3415 chars\n",
      "[load_pdfs]  Page 21: 2877 chars\n",
      "[load_pdfs]  Page 22: 1918 chars\n",
      "[load_pdfs]  Page 23: 2726 chars\n",
      "[load_pdfs]  Page 24: 2824 chars\n",
      "[load_pdfs]  Page 25: 3100 chars\n",
      "[load_pdfs]  Page 26: 1380 chars\n",
      "[load_pdfs]  Page 27: 71 chars\n",
      "[load_pdfs]  Page 28: 32 chars\n",
      "[load_pdfs]  Total US: 69823 chars\n",
      "\n",
      "[load_pdfs] Loading Japan from japan_AI_stratagy2022.pdf\n",
      "[load_pdfs]  Page 1: 121 chars\n",
      "[load_pdfs]  Page 2: 392 chars\n",
      "[load_pdfs]  Page 3: 2419 chars\n",
      "[load_pdfs]  Page 4: 1873 chars\n",
      "[load_pdfs]  Page 5: 2561 chars\n",
      "[load_pdfs]  Page 6: 2329 chars\n",
      "[load_pdfs]  Page 7: 1816 chars\n",
      "[load_pdfs]  Page 8: 2315 chars\n",
      "[load_pdfs]  Page 9: 2184 chars\n",
      "[load_pdfs]  Page 10: 1239 chars\n",
      "[load_pdfs]  Page 11: 2373 chars\n",
      "[load_pdfs]  Page 12: 2640 chars\n",
      "[load_pdfs]  Page 13: 2432 chars\n",
      "[load_pdfs]  Page 14: 2199 chars\n",
      "[load_pdfs]  Page 15: 2214 chars\n",
      "[load_pdfs]  Page 16: 2703 chars\n",
      "[load_pdfs]  Page 17: 2394 chars\n",
      "[load_pdfs]  Page 18: 2663 chars\n",
      "[load_pdfs]  Page 19: 2204 chars\n",
      "[load_pdfs]  Page 20: 1613 chars\n",
      "[load_pdfs]  Page 21: 1865 chars\n",
      "[load_pdfs]  Page 22: 2445 chars\n",
      "[load_pdfs]  Page 23: 2057 chars\n",
      "[load_pdfs]  Page 24: 2089 chars\n",
      "[load_pdfs]  Page 25: 2033 chars\n",
      "[load_pdfs]  Page 26: 2056 chars\n",
      "[load_pdfs]  Page 27: 2030 chars\n",
      "[load_pdfs]  Page 28: 2030 chars\n",
      "[load_pdfs]  Page 29: 1871 chars\n",
      "[load_pdfs]  Page 30: 2581 chars\n",
      "[load_pdfs]  Page 31: 1775 chars\n",
      "[load_pdfs]  Page 32: 1186 chars\n",
      "[load_pdfs]  Page 33: 1236 chars\n",
      "[load_pdfs]  Page 34: 2347 chars\n",
      "[load_pdfs]  Page 35: 1381 chars\n",
      "[load_pdfs]  Page 36: 1237 chars\n",
      "[load_pdfs]  Page 37: 1895 chars\n",
      "[load_pdfs]  Page 38: 1082 chars\n",
      "[load_pdfs]  Page 39: 2016 chars\n",
      "[load_pdfs]  Page 40: 2004 chars\n",
      "[load_pdfs]  Page 41: 1914 chars\n",
      "[load_pdfs]  Page 42: 2474 chars\n",
      "[load_pdfs]  Page 43: 2129 chars\n",
      "[load_pdfs]  Page 44: 1301 chars\n",
      "[load_pdfs]  Page 45: 1275 chars\n",
      "[load_pdfs]  Page 46: 1760 chars\n",
      "[load_pdfs]  Total Japan: 88753 chars\n",
      "\n",
      "US: 69823 characters\n",
      "Japan: 88753 characters\n"
     ]
    }
   ],
   "source": [
    "# 関数呼び出しセル（load_pdfs 定義セルの直後に配置）\n",
    "texts = load_pdfs(pdf_paths)\n",
    "\n",
    "# 任意でテキスト長も確認\n",
    "for country, txt in texts.items():\n",
    "    print(f\"{country}: {len(txt)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef60b1ab-5a83-4bd5-8b97-e00a81e3f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: テキストのチャンク分割（tiktoken, 4000トークン）\n",
    "def chunk_texts(texts: dict, max_tokens: int = 4000) -> dict:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    chunked = {}\n",
    "    for country, text in texts.items():\n",
    "        tokens = enc.encode(text)\n",
    "        chunks = [\n",
    "            enc.decode(tokens[i : i + max_tokens])\n",
    "            for i in range(0, len(tokens), max_tokens)\n",
    "        ]\n",
    "        chunked[country] = chunks\n",
    "    return chunked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dfae51-cb6e-410e-84db-3a55f2e2ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US: 4 chunks\n",
      " First chunk length: 21464 chars\n",
      "\n",
      "Japan: 5 chunks\n",
      " First chunk length: 20212 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 呼び出し＆デバッグ出力\n",
    "text_chunks = chunk_texts(texts, max_tokens=4000)\n",
    "\n",
    "# 各国のチャンク数と最初のチャンクの文字数を確認\n",
    "for country, chunks in text_chunks.items():\n",
    "    print(f\"{country}: {len(chunks)} chunks\")\n",
    "    if chunks:\n",
    "        print(f\" First chunk length: {len(chunks[0])} chars\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd8fc360-939f-41fb-b1da-93fb9e282a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 カテゴリのキーワードとスコア\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_common_keywords(chunked_texts: dict, category: str, top_n: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    新SDK対応版：US/Japanのチャンクから「category」と同義のキーワード候補を抽出し、\n",
    "    両国共通の上位top_nを返す関数。パースを堅牢化。\n",
    "    \"\"\"\n",
    "    all_candidates = {\"US\": [], \"Japan\": []}\n",
    "\n",
    "    for country, chunks in chunked_texts.items():\n",
    "        for idx, chunk in enumerate(chunks, start=1):\n",
    "            print(f\"[extract_common_keywords] {country} chunk {idx}/{len(chunks)}\")\n",
    "            prompt = f\"\"\"\n",
    "You are a policy consultant with deep expertise in both AI regulation and the advancement of AI-driven innovation.\n",
    "You understand the balance between responsible governance and the strategic promotion of AI technologies.\n",
    "You provide nuanced advice that reflects both regulatory considerations and opportunities for AI adoption.\n",
    "You are analyzing a national AI policy document.\n",
    "Only output a JSON object without markdown or code fences.\n",
    "\n",
    "From the following text chunk—do not refer to or use any information outside this chunk—extract candidate keywords\n",
    "that are synonymous with \"{category}\"\n",
    "(including the term \"{category}\" itself if present), and return JSON:\n",
    "\n",
    "{{\n",
    "  \"candidates\": [\"keyword1\", \"keyword2\", ...]\n",
    "}}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "            # GPT呼び出し（新SDK）\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "            print(f\"[extract_common_keywords] response snippet: {content[:100].replace(chr(10), ' ')}...\")\n",
    "\n",
    "            # JSON部分だけ抽出\n",
    "            try:\n",
    "                body = re.search(r'\\{.*\\}', content, flags=re.DOTALL).group()\n",
    "                data = json.loads(body)\n",
    "                candidates = data.get(\"candidates\", [])\n",
    "            except Exception as e:\n",
    "                print(f\"[extract_common_keywords] JSON parse error: {e}\")\n",
    "                # フォールバック：配列内要素のみ抽出\n",
    "                match = re.search(r'\"candidates\"\\s*:\\s*\\[([^\\]]+)\\]', content)\n",
    "                if match:\n",
    "                    items = [s.strip().strip('\"') for s in match.group(1).split(',')]\n",
    "                    candidates = items\n",
    "                else:\n",
    "                    candidates = []\n",
    "\n",
    "            all_candidates[country].extend(candidates)\n",
    "\n",
    "    # 両国共通のキーワードを抽出し、上位top_nを返す\n",
    "    common = list(set(all_candidates[\"US\"]) & set(all_candidates[\"Japan\"]))\n",
    "    print(f\"[extract_common_keywords] common keywords: {common[:top_n]}\")\n",
    "    return common[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68ab5fe5-2d7b-4291-94d1-1512d8cb4f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3] Extracting regulation keywords…\n",
      "[extract_common_keywords] US chunk 1/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"regulation\", \"regulatory\", \"regulatory regime\", \"regulatory barriers\", \"regulato...\n",
      "[extract_common_keywords] US chunk 2/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"guidance\", \"authorities\", \"regulations\", \"rules\", \"law\", \"compliance\", \"mandate\"...\n",
      "[extract_common_keywords] US chunk 3/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"regulation\", \"guidance\", \"rules\", \"standards\", \"requirements\", \"laws\"] }...\n",
      "[extract_common_keywords] US chunk 4/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"regulations\", \"governance\", \"standards\", \"controls\", \"requirements\", \"protection...\n",
      "[extract_common_keywords] Japan chunk 1/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"governance\", \"legal systems\", \"policy obstacles\", \"institutional obstacles\", \"go...\n",
      "[extract_common_keywords] Japan chunk 2/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"governance\", \"policy\", \"measures\", \"requirements\", \"standards\", \"guidelines\", \"f...\n",
      "[extract_common_keywords] Japan chunk 3/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [] }...\n",
      "[extract_common_keywords] Japan chunk 4/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [] }...\n",
      "[extract_common_keywords] Japan chunk 5/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"governance\"] }...\n",
      "[extract_common_keywords] common keywords: ['requirements', 'standards', 'rules', 'regulation', 'governance', 'compliance', 'laws']\n",
      "[Step 3] regulation_keywords: ['requirements', 'standards', 'rules', 'regulation', 'governance', 'compliance', 'laws']\n",
      "\n",
      "[Step 3] Extracting promotion keywords…\n",
      "[extract_common_keywords] US chunk 1/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"promotion\", \"encourage\", \"enable\", \"drive\", \"support\", \"foster\", \"advance\", \"emp...\n",
      "[extract_common_keywords] US chunk 2/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [     \"advancement\",     \"prioritize\",     \"usher in\",     \"support\",     \"incenti...\n",
      "[extract_common_keywords] US chunk 3/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"promotion\", \"advancement\", \"drive\", \"export\", \"distribution\", \"diffusion\"] }...\n",
      "[extract_common_keywords] US chunk 4/4\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"facilitate\", \"encourage\", \"advocate\", \"induce\", \"promoting\"] }...\n",
      "[extract_common_keywords] Japan chunk 1/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [     \"promotion\",     \"promoted\",     \"promote\",     \"advancement\",     \"advancin...\n",
      "[extract_common_keywords] Japan chunk 2/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"promotion\", \"promoted\", \"promoting\", \"promote\"] }...\n",
      "[extract_common_keywords] Japan chunk 3/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [     \"promotion\",     \"promote\",     \"advancing\",     \"advancement\",     \"encoura...\n",
      "[extract_common_keywords] Japan chunk 4/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [     \"promotion\",     \"dissemination\",     \"strategic promotion\",     \"promote\", ...\n",
      "[extract_common_keywords] Japan chunk 5/5\n",
      "[extract_common_keywords] response snippet: {   \"candidates\": [\"deployment\", \"realization\", \"development\", \"implementation\"] }...\n",
      "[extract_common_keywords] common keywords: ['support', 'foster', 'promote', 'advancement', 'encourage', 'promotion', 'facilitate', 'promoting', 'accelerate']\n",
      "[Step 3] promotion_keywords: ['support', 'foster', 'promote', 'advancement', 'encourage', 'promotion', 'facilitate', 'promoting', 'accelerate']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 呼び出し＆デバッグ出力\n",
    "print(\"[Step 3] Extracting regulation keywords…\")\n",
    "regulation_keywords = extract_common_keywords(text_chunks, \"regulation\", top_n=20)\n",
    "print(f\"[Step 3] regulation_keywords: {regulation_keywords}\\n\")\n",
    "\n",
    "print(\"[Step 3] Extracting promotion keywords…\")\n",
    "promotion_keywords = extract_common_keywords(text_chunks, \"promotion\", top_n=20)\n",
    "print(f\"[Step 3] promotion_keywords: {promotion_keywords}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c4b1e5d-ca06-4efa-a7d1-7d780fb83594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: スコア計算のみ行う関数（CSV保存ナシ）\n",
    "def count_and_score(texts: dict, regulation_keywords: list, promotion_keywords: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    textsとキーワードリストを小文字化した上で出現回数を集計し、\n",
    "    スコアを計算してDataFrameで返す関数。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for country, text in texts.items():\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # 規制キーワードの小文字版リスト\n",
    "        reg_keywords_lower = [kw.lower() for kw in regulation_keywords]\n",
    "        pro_keywords_lower = [kw.lower() for kw in promotion_keywords]\n",
    "\n",
    "        # 出現回数を .count() でカウント\n",
    "        reg_count = sum(text_lower.count(kw) for kw in reg_keywords_lower)\n",
    "        pro_count = sum(text_lower.count(kw) for kw in pro_keywords_lower)\n",
    "\n",
    "        # スコア計算\n",
    "        score = 0 if (reg_count + pro_count) == 0 else (pro_count - reg_count) / (pro_count + reg_count)\n",
    "\n",
    "        results.append({\n",
    "            \"country\": country,\n",
    "            \"regulation_count\": reg_count,\n",
    "            \"promotion_count\": pro_count,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc7fe947-311f-4c89-923b-270966600385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>regulation_count</th>\n",
       "      <th>promotion_count</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>103</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.420690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>29</td>\n",
       "      <td>101</td>\n",
       "      <td>0.553846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  regulation_count  promotion_count     score\n",
       "0      US               103               42 -0.420690\n",
       "1   Japan                29              101  0.553846"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: スコアデバック\n",
    "df_scores = count_and_score(texts, regulation_keywords, promotion_keywords)\n",
    "df_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1e215af7-6403-4b3a-8fe6-283c7d68312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: スコアダウンロード\n",
    "df_scores.to_csv(\"sentiment_scores.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf6ab6f8-4c9f-42a1-a4b9-84b4a19f13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>country</th>\n",
       "      <th>keyword</th>\n",
       "      <th>category</th>\n",
       "      <th>Japan</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accelerate</td>\n",
       "      <td>promotion</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advancement</td>\n",
       "      <td>promotion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>compliance</td>\n",
       "      <td>regulation</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encourage</td>\n",
       "      <td>promotion</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facilitate</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>foster</td>\n",
       "      <td>promotion</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>governance</td>\n",
       "      <td>regulation</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>laws</td>\n",
       "      <td>regulation</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>promote</td>\n",
       "      <td>promotion</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>promoting</td>\n",
       "      <td>promotion</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>promotion</td>\n",
       "      <td>promotion</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>regulation</td>\n",
       "      <td>regulation</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>requirements</td>\n",
       "      <td>regulation</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rules</td>\n",
       "      <td>regulation</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>standards</td>\n",
       "      <td>regulation</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>support</td>\n",
       "      <td>promotion</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "country       keyword    category  Japan  US\n",
       "0          accelerate   promotion      7   9\n",
       "1         advancement   promotion      1   1\n",
       "2          compliance  regulation      0   2\n",
       "3           encourage   promotion      1   6\n",
       "4          facilitate   promotion      0   2\n",
       "5              foster   promotion      1   2\n",
       "6          governance  regulation      2   8\n",
       "7                laws  regulation      0   3\n",
       "8             promote   promotion     29  10\n",
       "9           promoting   promotion     12   3\n",
       "10          promotion   promotion     19   0\n",
       "11         regulation  regulation      0   3\n",
       "12       requirements  regulation      1   6\n",
       "13              rules  regulation      3   5\n",
       "14          standards  regulation      0  17\n",
       "15            support   promotion     27  10"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4　全キーワードとカテゴリをまとめる\n",
    "all_keywords = regulation_keywords + promotion_keywords\n",
    "\n",
    "# 国別にキーワード出現回数を集計\n",
    "rows = []\n",
    "for kw in all_keywords:\n",
    "    for country in texts:\n",
    "        count = len(re.findall(rf\"\\b{kw}\\b\", texts[country], flags=re.IGNORECASE))\n",
    "        rows.append({\n",
    "            \"keyword\": kw,\n",
    "            \"category\": \"regulation\" if kw in regulation_keywords else \"promotion\",\n",
    "            \"country\": country,\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "# rows を作成したあとに DataFrame を生成\n",
    "df_keyword_counts = pd.DataFrame(rows)\n",
    "\n",
    "# そして pivot_table を作成\n",
    "pivot_table = pd.pivot_table(\n",
    "    df_keyword_counts, \n",
    "    index=[\"keyword\", \"category\"],\n",
    "    columns=\"country\",\n",
    "    values=\"count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "pivot_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e944e029-3bdb-44c5-ba03-3c2cb306e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4　キーワードCSV出力セル\n",
    "pivot_table.to_csv(\n",
    "    \"keyword_pivot.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"  # Excel 互換のUTF-8 BOM付き\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52979117-bd69-4ce0-bb82-a8123b296b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: 共通 regulation/promption キーワード抽出\n",
    "regulation_keywords = extract_common_keywords(text_chunks, \"regulation\", top_n=10)\n",
    "promotion_keywords = extract_common_keywords(text_chunks, \"promotion\", top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21d215c6-19e4-47bc-a29b-d60e0a98a92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4: カウント＆スコア算出\u001b[39;00m\n\u001b[32m      2\u001b[39m df_scores = count_and_score(texts, regulation_keywords, promotion_keywords)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdf_score\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_score' is not defined"
     ]
    }
   ],
   "source": [
    "# 4: カウント＆スコア算出\n",
    "df_scores = count_and_score(texts, regulation_keywords, promotion_keywords)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22a1708e-ef13-4e7c-8959-31ac74096c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP5] START US: 4 chunks\n",
      "[STEP5] US chunk 1/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI innovation\",\n",
      "    \"AI adoption\",\n",
      "    \"AI-related Federal ...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 2/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI-related training\",\n",
      "    \"AI adoption\",\n",
      "    \"AI systems\",\n",
      "...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 3/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"deepfake standard\",\n",
      "    \"Federal Rules of Evidence\",\n",
      "    \"e...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 4/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI governance frameworks\",\n",
      "    \"facial recognition\",\n",
      "    \"s...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US regulation_targets: ['\\n    \"AI innovation\",\\n    \"AI adoption\",\\n    \"AI-related Federal funding\",\\n    \"state AI regulations\",\\n    \"AI development\",\\n    \"AI deployment\",\\n    \"AI systems\",\\n    \"AI procurement\",\\n    \"AI regulatory climate\",\\n    \"AI-related discretionary funding programs\",\\n    \"AI-related investigations\",\\n    \"AI-related final orders\",\\n    \"AI-related consent decrees\",\\n    \"AI-related injunctions\"\\n  ', '\\n    \"AI-related training\",\\n    \"AI adoption\",\\n    \"AI systems\",\\n    \"AI use-cases\",\\n    \"AI innovations\",\\n    \"AI-generated media\"\\n  ', '\\n    \"deepfake standard\",\\n    \"Federal Rules of Evidence\",\\n    \"environmental permitting\",\\n    \"Clean Water Act Section 404 permit\",\\n    \"Clean Air Act\",\\n    \"Comprehensive Environmental Response, Compensation, and Liability Act\",\\n    \"security guardrails\",\\n    \"AI Information Sharing and Analysis Center\",\\n    \"AI-specific vulnerabilities\",\\n    \"AI Incident Response\"\\n  ', '\\n    \"AI governance frameworks\",\\n    \"facial recognition\",\\n    \"surveillance\",\\n    \"export controls\",\\n    \"semiconductor manufacturing\",\\n    \"technology protection measures\",\\n    \"frontier AI systems\",\\n    \"nucleic acid synthesis\"\\n  ']\n",
      "[STEP5] US promotion_targets: ['\\n    \"AI innovation\",\\n    \"Open-Source AI\",\\n    \"Open-Weight AI\",\\n    \"AI adoption\",\\n    \"AI skill development\",\\n    \"AI literacy\",\\n    \"AI-enabled science\",\\n    \"AI infrastructure\",\\n    \"AI incident response\",\\n    \"AI diplomacy\",\\n    \"AI security\",\\n    \"AI research investments\",\\n    \"AI Centers of Excellence\",\\n    \"AI tools\",\\n    \"AI standards\"\\n  ', '\\n    \"AI literacy\",\\n    \"AI skill development\",\\n    \"AI Workforce Research Hub\",\\n    \"next-generation technologies\",\\n    \"AI-enabled science\",\\n    \"AI interpretability\",\\n    \"AI evaluations\",\\n    \"AI adoption in government\",\\n    \"AI within the Department of Defense\",\\n    \"AI research, development, and talent building\"\\n  ', '\\n    \"American AI Infrastructure\",\\n    \"energy generation\",\\n    \"data centers\",\\n    \"semiconductor manufacturing\",\\n    \"AI computing stack\",\\n    \"AI-enabled cyberdefensive tools\",\\n    \"Secure-By-Design AI Technologies\",\\n    \"AI Assurance\",\\n    \"AI Incident Response actions\",\\n    \"American AI systems\"\\n  ', '\\n    \"AI development\",\\n    \"innovation\",\\n    \"shared values\",\\n    \"AI global alliance\",\\n    \"plurilateral controls\",\\n    \"biosecurity\",\\n    \"data sharing\"\\n  ']\n",
      "[STEP5] US unique_terms: ['\\n    \"transformative\",\\n    \"unquestioned\",\\n    \"unchallenged\",\\n    \"dismantle\",\\n    \"onerous\",\\n    \"hyperscalers\",\\n    \"geostrategic\",\\n    \"sandboxes\",\\n    \"pathways\",\\n    \"apprenticeships\",\\n    \"unencumbered\",\\n    \"bureaucratic\",\\n    \"misinformation\",\\n    \"hyperscalers\",\\n    \"geostrategic\"\\n  ', '\\n    \"educational assistance\",\\n    \"tax-free reimbursement\",\\n    \"job displacement\",\\n    \"upskill\",\\n    \"workforce intermediaries\",\\n    \"industrial renaissance\",\\n    \"translational manufacturing\",\\n    \"cloud-enabled labs\",\\n    \"Focused-Research Organizations\",\\n    \"whole-genome sequencing\",\\n    \"paradigm shift\",\\n    \"adversarial robustness\",\\n    \"hackathon\",\\n    \"interoperable techniques\",\\n    \"procurement toolbox\",\\n    \"High Impact Service Providers\",\\n    \"back-office operations\",\\n    \"Autonomous Systems Virtual Proving Ground\",\\n    \"malicious deepfakes\",\\n    \"forensic evidence\"\\n  ', '\\n    \"deepfake\",\\n    \"adjudications\",\\n    \"permitting\",\\n    \"grid\",\\n    \"dispatchable\",\\n    \"geothermal\",\\n    \"fission\",\\n    \"fusion\",\\n    \"semiconductor\",\\n    \"workforce\",\\n    \"cyberdefensive\",\\n    \"resilient\",\\n    \"incident response\",\\n    \"fly-away kits\",\\n    \"diplomacy\"\\n  ', '\\n    \"geostrategic\",\\n    \"plurilateral\",\\n    \"backdoors\",\\n    \"malign\",\\n    \"pathbreaking\",\\n    \"synthesize\",\\n    \"biomolecules\",\\n    \"attestation\",\\n    \"dynamism\",\\n    \"adversaries\"\\n  ']\n",
      "\n",
      "[STEP5] START Japan: 5 chunks\n",
      "[STEP5] Japan chunk 1/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"cybersecurity\",\n",
      "    \"AI ethics\",\n",
      "    \"data-related infrastr...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 2/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI\",\n",
      "    \"digitization\",\n",
      "    \"cybersecurity\",\n",
      "    \"data flo...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 3/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"data handling rules\",\n",
      "    \"security risks\",\n",
      "    \"reliabilit...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 4/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI utilization\",\n",
      "    \"data collaboration\",\n",
      "    \"AI introduc...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 5/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI systems\",\n",
      "    \"cybersecurity\",\n",
      "    \"AI ethics\",\n",
      "    \"inf...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan regulation_targets: ['\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    \"data-related infrastructure\",\\n    \"legal systems\",\\n    \"emergency situations\",\\n    \"institutional and policy obstacles\"\\n  ', '\\n    \"AI\",\\n    \"digitization\",\\n    \"cybersecurity\",\\n    \"data flows\",\\n    \"social infrastructure\",\\n    \"economic activity\",\\n    \"supply chains\",\\n    \"urban planning\",\\n    \"resource recycling\"\\n  ', '\\n    \"data handling rules\",\\n    \"security risks\",\\n    \"reliability of AI\",\\n    \"bias\",\\n    \"confidential data\"\\n  ', '\\n    \"AI utilization\",\\n    \"data collaboration\",\\n    \"AI introduction guidelines\",\\n    \"cyber / physical security measures framework\",\\n    \"AI-related core centers\",\\n    \"AI R & D network\",\\n    \"AI research and development\",\\n    \"AI data-related infrastructure\",\\n    \"AI Social Principles\",\\n    \"international standardization activities\"\\n  ', '\\n    \"AI systems\",\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    \"infrastructure\",\\n    \"communications networks\",\\n    \"data\",\\n    \"core infrastructure\",\\n    \"electricity\",\\n    \"water supply\",\\n    \"sewerage\",\\n    \"roads\",\\n    \"waste treatment\",\\n    \"governance conditions\"\\n  ']\n",
      "[STEP5] Japan promotion_targets: ['\\n    \"education reform\",\\n    \"human resources development\",\\n    \"industrial competitiveness\",\\n    \"sustainable society\",\\n    \"digital transformation\",\\n    \"AI literacy\",\\n    \"international cooperation\"\\n  ', '\\n    \"Planetary Resilience\",\\n    \"National Resilience\",\\n    \"technological innovations\",\\n    \"sustainability\",\\n    \"AI for Good\",\\n    \"digital twins\",\\n    \"global leadership\",\\n    \"biodiversity\",\\n    \"Nature-based Solutions\"\\n  ', '\\n    \"AI implementation\",\\n    \"AI utilization\",\\n    \"education reform\",\\n    \"digital transformation\",\\n    \"international brain circulation\"\\n  ', '\\n    \"STEAM Education\",\\n    \"cross-curricular learning\",\\n    \"ICT infrastructure\",\\n    \"mathematics, data science and AI education\",\\n    \"recurrent education\",\\n    \"international students\",\\n    \"AI research base\",\\n    \"emergent research\",\\n    \"AI venture companies\",\\n    \"AI-related startups\"\\n  ', '\\n    \"intelligent functions\",\\n    \"technology development\",\\n    \"medical care\",\\n    \"agriculture\",\\n    \"materials\",\\n    \"logistics\",\\n    \"manufacturing facilities\",\\n    \"education\",\\n    \"data science literacy\",\\n    \"AI literacy\",\\n    \"cross-curricular education\"\\n  ']\n",
      "[STEP5] Japan unique_terms: ['\\n    \"resilience\",\\n    \"sustainability\",\\n    \"diversity\",\\n    \"multilateral framework\",\\n    \"human dignity\",\\n    \"labor productivity\",\\n    \"service platforms\",\\n    \"social acceptance\",\\n    \"geopolitical risks\",\\n    \"biodiversity\"\\n  ', '\\n    \"crustal movement\",\\n    \"Climate Departure\",\\n    \"carbon neutrality\",\\n    \"Visionary Meeting\",\\n    \"Moonshot Research\",\\n    \"lockdown\",\\n    \"economic rationality\",\\n    \"telemedicine\",\\n    \"personalized medicine\",\\n    \"Inclusive Wealth\",\\n    \"Ecosystem Services\",\\n    \"Nature-Positive\",\\n    \"terraformation\",\\n    \"Explainable AI\",\\n    \"Federated Learning\",\\n    \"cybernetic resilience\"\\n  ', '\\n    \"resilience\",\\n    \"panoramic perspective\",\\n    \"grand challenge\",\\n    \"Explainable AI\",\\n    \"data economic zone\"\\n  ', '\\n    \"recurrent education\",\\n    \"cross-curricular learning\",\\n    \"Society 5.0\",\\n    \"sustainable society\",\\n    \"human resources\",\\n    \"creativity\",\\n    \"innovation\",\\n    \"critical thinking\",\\n    \"liberal arts education\",\\n    \"trust infrastructure\"\\n  ', '\\n    \"complex systems\",\\n    \"large-scale data\",\\n    \"widespread acceptance\",\\n    \"integrated manner\",\\n    \"Physical Real World\",\\n    \"interaction\",\\n    \"nominal labour productivity\",\\n    \"purchasing power parity\",\\n    \"climate departure\",\\n    \"digital twins\",\\n    \"simulation\",\\n    \"topography\",\\n    \"scramble rescue\",\\n    \"bottlenecks\",\\n    \"evacuation sites\",\\n    \"distributed information processing\",\\n    \"sensing functions\",\\n    \"natural disaster\",\\n    \"technical resources\",\\n    \"human resources\",\\n    \"virtual\",\\n    \"circular economy\",\\n    \"environmental risks\",\\n    \"biodiversity\",\\n    \"urban transformation\",\\n    \"cross-curricular\",\\n    \"intellectual abilities\"\\n  ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Step 5: 国ごとの特徴キーワード抽出（Prompt の３リストをパース）\n",
    "def extract_policy_keywords(chunked_texts: dict, top_n: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    各国のテキストチャンクから下記を GPT-4o で抽出し、\n",
    "    ・regulation_targets\n",
    "    ・promotion_targets\n",
    "    ・unique_terms\n",
    "    の３つを辞書で返す関数。\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for country, chunks in chunked_texts.items():\n",
    "        print(f\"[STEP5] START {country}: {len(chunks)} chunks\")\n",
    "        regs_all = []\n",
    "        promos_all = []\n",
    "        uniques_all = []\n",
    "\n",
    "        for idx, chunk in enumerate(chunks, start=1):\n",
    "            print(f\"[STEP5] {country} chunk {idx}/{len(chunks)}\")\n",
    "            prompt = f\"\"\"\n",
    "You are an expert in policy text analysis.\n",
    "\n",
    "Given the following policy document text chunk, extract three distinct lists of English keywords:\n",
    "1. “Regulation Targets”: terms that indicate what is being regulated.\n",
    "2. “Promotion Targets”: terms that indicate what is being promoted.\n",
    "3. “Unique Terms”: unusually significant words computed by a TF-IDF–like approach, excluding proper nouns such as “AI”, country names, and organization names.\n",
    "\n",
    "Return a JSON object with these three arrays:\n",
    "{{\n",
    "  \"regulation_targets\": [...],\n",
    "  \"promotion_targets\": [...],\n",
    "  \"unique_terms\": [...]\n",
    "}}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
    "                    {\"role\":\"user\",  \"content\":prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "            print(f\"[STEP5] snippet: {content[:100]}...\")\n",
    "\n",
    "            # JSON パース\n",
    "            try:\n",
    "                data = json.loads(content)\n",
    "                regs = data.get(\"regulation_targets\", [])\n",
    "                promos = data.get(\"promotion_targets\", [])\n",
    "                uniques = data.get(\"unique_terms\", [])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[STEP5] JSON error: {e}\")\n",
    "                # フォールバック：キーごとに正規表現で抜き出す\n",
    "                regs = re.findall(r'\"regulation_targets\"\\s*:\\s*\\[([^\\]]+)\\]', content)\n",
    "                promos = re.findall(r'\"promotion_targets\"\\s*:\\s*\\[([^\\]]+)\\]', content)\n",
    "                uniques = re.findall(r'\"unique_terms\"\\s*:\\s*\\[([^\\]]+)\\]', content)\n",
    "                # 上記は文字列なのでカンマで分割＆クオート除去などの後処理が必要です\n",
    "\n",
    "            regs_all.extend(regs)\n",
    "            promos_all.extend(promos)\n",
    "            uniques_all.extend(uniques)\n",
    "\n",
    "        # 重複除去しつつ先頭 top_n を切り出し\n",
    "        regs_final   = list(dict.fromkeys(regs_all))[:top_n]\n",
    "        promos_final = list(dict.fromkeys(promos_all))[:top_n]\n",
    "        uniques_final= list(dict.fromkeys(uniques_all))[:top_n]\n",
    "\n",
    "        print(f\"[STEP5] {country} regulation_targets: {regs_final}\")\n",
    "        print(f\"[STEP5] {country} promotion_targets: {promos_final}\")\n",
    "        print(f\"[STEP5] {country} unique_terms: {uniques_final}\\n\")\n",
    "\n",
    "        results[country] = {\n",
    "            \"regulation_targets\": regs_final,\n",
    "            \"promotion_targets\": promos_final,\n",
    "            \"unique_terms\": uniques_final\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# 実行例\n",
    "policy_keywords = extract_policy_keywords(text_chunks, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f28a9bc-50e9-46b0-a688-17b226b2f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP5] START US: 4 chunks\n",
      "[STEP5] US chunk 1/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI innovation\",\n",
      "    \"AI adoption\",\n",
      "    \"AI-related Federal ...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 2/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI-related training\",\n",
      "    \"AI adoption\",\n",
      "    \"AI systems\",\n",
      "...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 3/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"deepfake standard\",\n",
      "    \"Federal Rules of Evidence\",\n",
      "    \"e...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US chunk 4/4\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI governance frameworks\",\n",
      "    \"facial recognition\",\n",
      "    \"s...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] US regulation_targets: ['\\n    \"AI innovation\",\\n    \"AI adoption\",\\n    \"AI-related Federal funding\",\\n    \"state AI regulations\",\\n    \"Federal procurement guidelines\",\\n    \"AI regulatory climate\",\\n    \"AI-related discretionary funding programs\",\\n    \"AI systems\",\\n    \"AI tools\",\\n    \"AI Centers of Excellence\",\\n    \"AI evaluation initiatives\",\\n    \"AI standards\",\\n    \"AI net assessments\"\\n  ', '\\n    \"AI-related training\",\\n    \"AI adoption\",\\n    \"AI systems\",\\n    \"AI use-cases\",\\n    \"AI innovations\",\\n    \"AI-generated media\"\\n  ', '\\n    \"deepfake standard\",\\n    \"Federal Rules of Evidence\",\\n    \"environmental permitting\",\\n    \"Clean Water Act Section 404 permit\",\\n    \"Clean Air Act\",\\n    \"Comprehensive Environmental Response, Compensation, and Liability Act\",\\n    \"security guardrails\",\\n    \"AI Information Sharing and Analysis Center\",\\n    \"AI-specific vulnerabilities\",\\n    \"AI Incident Response actions\"\\n  ', '\\n    \"AI governance frameworks\",\\n    \"facial recognition\",\\n    \"surveillance\",\\n    \"export controls\",\\n    \"semiconductor manufacturing\",\\n    \"technology protection measures\",\\n    \"frontier AI systems\",\\n    \"nucleic acid synthesis\"\\n  ']\n",
      "[STEP5] US promotion_targets: ['\\n    \"AI innovation\",\\n    \"Open-Source and Open-Weight AI\",\\n    \"AI adoption\",\\n    \"AI skill development\",\\n    \"AI literacy\",\\n    \"AI-enabled science\",\\n    \"AI infrastructure\",\\n    \"American workers\",\\n    \"AI research investments\",\\n    \"AI breakthroughs\",\\n    \"AI-driven economy\"\\n  ', '\\n    \"AI literacy\",\\n    \"AI skill development\",\\n    \"AI Workforce Research Hub\",\\n    \"next-generation technologies\",\\n    \"AI-enabled science\",\\n    \"AI interpretability\",\\n    \"AI evaluations\",\\n    \"AI adoption in government\",\\n    \"AI within the Department of Defense\",\\n    \"AI research, development, and talent building\"\\n  ', '\\n    \"American AI Infrastructure\",\\n    \"energy generation\",\\n    \"data centers\",\\n    \"semiconductor manufacturing\",\\n    \"AI computing stack\",\\n    \"AI-enabled cyberdefensive tools\",\\n    \"resilient and secure AI development\",\\n    \"AI Incident Response\",\\n    \"American AI systems\",\\n    \"AI alliance\"\\n  ', '\\n    \"AI development\",\\n    \"innovation\",\\n    \"shared values\",\\n    \"AI global alliance\",\\n    \"plurilateral controls\",\\n    \"biosecurity\"\\n  ']\n",
      "[STEP5] US unique_terms: ['\\n    \"frontier\",\\n    \"transformative\",\\n    \"unencumbered\",\\n    \"hyperscalers\",\\n    \"geostrategic\",\\n    \"deregulation\",\\n    \"bureaucratic\",\\n    \"misinformation\",\\n    \"sandbox\",\\n    \"compute\",\\n    \"hyperscalers\",\\n    \"spot and forward markets\",\\n    \"regulatory sandboxes\",\\n    \"AI Centers of Excellence\",\\n    \"AI literacy\"\\n  ', '\\n    \"autonomous drones\",\\n    \"self-driving cars\",\\n    \"robotics\",\\n    \"novel materials\",\\n    \"scientific datasets\",\\n    \"whole-genome sequencing\",\\n    \"paradigm shift\",\\n    \"adversarial robustness\",\\n    \"hackathon\",\\n    \"testbeds\",\\n    \"High Impact Service Providers\",\\n    \"deepfakes\",\\n    \"forensic evidence\"\\n  ', '\\n    \"deepfake\",\\n    \"adjudications\",\\n    \"permitting\",\\n    \"grid\",\\n    \"dispatchable\",\\n    \"geothermal\",\\n    \"fission\",\\n    \"fusion\",\\n    \"semiconductor\",\\n    \"workforce\",\\n    \"apprenticeships\",\\n    \"cyberdefensive\",\\n    \"adversarial\",\\n    \"resilient\",\\n    \"incident response\",\\n    \"diplomacy\",\\n    \"export\"\\n  ', '\\n    \"geostrategic\",\\n    \"plurilateral\",\\n    \"backdoors\",\\n    \"malicious\",\\n    \"biomolecules\",\\n    \"pathbreaking\",\\n    \"dynamism\",\\n    \"attestation\",\\n    \"synthesize\"\\n  ']\n",
      "\n",
      "[STEP5] START Japan: 5 chunks\n",
      "[STEP5] Japan chunk 1/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"cybersecurity\",\n",
      "    \"AI ethics\",\n",
      "    \"data-related infrastr...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 2/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI\",\n",
      "    \"digitization\",\n",
      "    \"cybersecurity\",\n",
      "    \"data flo...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 3/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"data handling rules\",\n",
      "    \"security risks\",\n",
      "    \"reliabilit...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 4/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI utilization\",\n",
      "    \"data collaboration\",\n",
      "    \"cyber / phy...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan chunk 5/5\n",
      "[STEP5] snippet: ```json\n",
      "{\n",
      "  \"regulation_targets\": [\n",
      "    \"AI systems\",\n",
      "    \"cybersecurity\",\n",
      "    \"AI ethics\",\n",
      "    \"inf...\n",
      "[STEP5] JSON error: Expecting value: line 1 column 1 (char 0)\n",
      "[STEP5] Japan regulation_targets: ['\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    \"data-related infrastructure\",\\n    \"legal systems\",\\n    \"emergency situations\"\\n  ', '\\n    \"AI\",\\n    \"digitization\",\\n    \"cybersecurity\",\\n    \"data flows\",\\n    \"social infrastructure\",\\n    \"economic activity\",\\n    \"supply chains\",\\n    \"urban planning\",\\n    \"resource recycling\"\\n  ', '\\n    \"data handling rules\",\\n    \"security risks\",\\n    \"reliability of AI\",\\n    \"ethical considerations\",\\n    \"AI utilization cycle\"\\n  ', '\\n    \"AI utilization\",\\n    \"data collaboration\",\\n    \"cyber / physical security measures\",\\n    \"AI introduction guidelines\",\\n    \"AI-related core centers\",\\n    \"AI R & D network\",\\n    \"AI research and development\",\\n    \"AI data-related infrastructure\",\\n    \"AI Social Principles\",\\n    \"international standardization activities\"\\n  ', '\\n    \"AI systems\",\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    \"infrastructure\",\\n    \"communications networks\",\\n    \"data\",\\n    \"core infrastructure\",\\n    \"electricity\",\\n    \"water supply\",\\n    \"sewerage\",\\n    \"roads\",\\n    \"waste treatment\"\\n  ']\n",
      "[STEP5] Japan promotion_targets: ['\\n    \"education reform\",\\n    \"human resources development\",\\n    \"industrial competitiveness\",\\n    \"sustainable society\",\\n    \"AI-related research and development\"\\n  ', '\\n    \"Planetary Resilience\",\\n    \"National Resilience\",\\n    \"Digital Twins\",\\n    \"technological innovations\",\\n    \"sustainability\",\\n    \"AI for Good\",\\n    \"renewable energy\",\\n    \"telemedicine\",\\n    \"personalized medicine\",\\n    \"Nature-based Solutions\"\\n  ', '\\n    \"AI implementation\",\\n    \"AI applications\",\\n    \"AI utilization\",\\n    \"education reform\",\\n    \"international brain circulation\"\\n  ', '\\n    \"STEAM Education\",\\n    \"ICT infrastructure\",\\n    \"mathematics, data science and AI education\",\\n    \"recurrent education\",\\n    \"international students\",\\n    \"AI research base\",\\n    \"emergent research\",\\n    \"AI venture companies\",\\n    \"AI-related startups\",\\n    \"AI Social Principles\"\\n  ', '\\n    \"intelligent functions\",\\n    \"society\",\\n    \"industry\",\\n    \"everyday life\",\\n    \"scientific research\",\\n    \"technology development\",\\n    \"medical care\",\\n    \"agriculture\",\\n    \"materials\",\\n    \"logistics\",\\n    \"manufacturing facilities\",\\n    \"Digital Twins\",\\n    \"sensing functions\",\\n    \"technical resources\",\\n    \"human resources\",\\n    \"education\",\\n    \"data science literacy\",\\n    \"AI literacy\"\\n  ']\n",
      "[STEP5] Japan unique_terms: ['\\n    \"resilience\",\\n    \"sustainability\",\\n    \"diversity\",\\n    \"multilateral framework\",\\n    \"digital transformation\"\\n  ', '\\n    \"crustal movement\",\\n    \"Climate Departure\",\\n    \"inequality\",\\n    \"biodiversity\",\\n    \"telemedicine\",\\n    \"Explainable AI\",\\n    \"Federated Learning\",\\n    \"cybernetic resilience\",\\n    \"flywheel\",\\n    \"Nature-Positive Economy\"\\n  ', '\\n    \"resilience\",\\n    \"diversity\",\\n    \"sustainability\",\\n    \"Explainable AI\",\\n    \"cyberspace\",\\n    \"digital transformation\",\\n    \"deep learning\",\\n    \"data economic zone\",\\n    \"digital twins\",\\n    \"brain circulation\",\\n    \"fusion\",\\n    \"panoramic perspective\",\\n    \"grand challenge\",\\n    \"black box\",\\n    \"loop\",\\n    \"integration\"\\n  ', '\\n    \"recurrent education\",\\n    \"cross-curricular learning\",\\n    \"Society 5.0\",\\n    \"reading, writing and abacus\",\\n    \"liberal arts education\",\\n    \"critical thinking skills\",\\n    \"regional revitalization\",\\n    \"trusted quality AI\",\\n    \"cyber / physical security measures\",\\n    \"digital government\",\\n    \"AI literacy\",\\n    \"GPAI\",\\n    \"AI Social Principles\"\\n  ', '\\n    \"complex systems\",\\n    \"large-scale data\",\\n    \"widespread acceptance\",\\n    \"integrated manner\",\\n    \"Physical Real World\",\\n    \"interaction\",\\n    \"cyberspace\",\\n    \"nominal labour productivity\",\\n    \"purchasing power parity\",\\n    \"climate departure\",\\n    \"forced confinement\",\\n    \"disaster\",\\n    \"simulation\",\\n    \"topography\",\\n    \"bottlenecks\",\\n    \"evacuation\",\\n    \"governance\",\\n    \"feedback\",\\n    \"distributed information processing\",\\n    \"robust\",\\n    \"natural disaster\",\\n    \"reflection capabilities\",\\n    \"cross-curricular education\",\\n    \"intellectual abilities\",\\n    \"general knowledge\",\\n    \"decision making\"\\n  ']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>regulation_targets</th>\n",
       "      <th>promotion_targets</th>\n",
       "      <th>unique_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>\\n    \"AI innovation\",\\n    \"AI adoption\",\\n  ...</td>\n",
       "      <td>\\n    \"AI innovation\",\\n    \"Open-Source and O...</td>\n",
       "      <td>\\n    \"frontier\",\\n    \"transformative\",\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    ...</td>\n",
       "      <td>\\n    \"education reform\",\\n    \"human resource...</td>\n",
       "      <td>\\n    \"resilience\",\\n    \"sustainability\",\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                 regulation_targets  \\\n",
       "0      US  \\n    \"AI innovation\",\\n    \"AI adoption\",\\n  ...   \n",
       "1   Japan  \\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    ...   \n",
       "\n",
       "                                   promotion_targets  \\\n",
       "0  \\n    \"AI innovation\",\\n    \"Open-Source and O...   \n",
       "1  \\n    \"education reform\",\\n    \"human resource...   \n",
       "\n",
       "                                        unique_terms  \n",
       "0  \\n    \"frontier\",\\n    \"transformative\",\\n    ...  \n",
       "1  \\n    \"resilience\",\\n    \"sustainability\",\\n  ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Step 5 の呼び出し結果を受け取る（関数名は例です） ---\n",
    "policy_keywords = extract_policy_keywords(text_chunks, top_n=10)\n",
    "\n",
    "# 辞書からそれぞれのリストを取り出す\n",
    "regulation_targets  = {c: policy_keywords[c][\"regulation_targets\"]  for c in policy_keywords}\n",
    "promotion_targets   = {c: policy_keywords[c][\"promotion_targets\"]   for c in policy_keywords}\n",
    "unique_terms        = {c: policy_keywords[c][\"unique_terms\"]        for c in policy_keywords}\n",
    "\n",
    "# --- Step 6: CSV出力（Updated） ---\n",
    "import pandas as pd\n",
    "\n",
    "df_keywords = pd.DataFrame([\n",
    "    {\n",
    "        \"country\": country,\n",
    "        \"regulation_targets\": \", \".join(regulation_targets[country]),\n",
    "        \"promotion_targets\": \", \".join(promotion_targets[country]),\n",
    "        \"unique_terms\": \", \".join(unique_terms[country])\n",
    "    }\n",
    "    for country in texts\n",
    "])\n",
    "\n",
    "# DataFrame 確認\n",
    "display(df_keywords)\n",
    "\n",
    "# CSV 保存\n",
    "df_keywords.to_csv(\"ai_policy_keywords.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96386a5d-bba9-49e4-aa23-161e17aa46c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>regulation_targets</th>\n",
       "      <th>promotion_targets</th>\n",
       "      <th>unique_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>\\n    \"AI innovation\",\\n    \"AI adoption\",\\n  ...</td>\n",
       "      <td>\\n    \"AI innovation\",\\n    \"Open-Source and O...</td>\n",
       "      <td>\\n    \"frontier\",\\n    \"transformative\",\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>\\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    ...</td>\n",
       "      <td>\\n    \"education reform\",\\n    \"human resource...</td>\n",
       "      <td>\\n    \"resilience\",\\n    \"sustainability\",\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                 regulation_targets  \\\n",
       "0      US  \\n    \"AI innovation\",\\n    \"AI adoption\",\\n  ...   \n",
       "1   Japan  \\n    \"cybersecurity\",\\n    \"AI ethics\",\\n    ...   \n",
       "\n",
       "                                   promotion_targets  \\\n",
       "0  \\n    \"AI innovation\",\\n    \"Open-Source and O...   \n",
       "1  \\n    \"education reform\",\\n    \"human resource...   \n",
       "\n",
       "                                        unique_terms  \n",
       "0  \\n    \"frontier\",\\n    \"transformative\",\\n    ...  \n",
       "1  \\n    \"resilience\",\\n    \"sustainability\",\\n  ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: CSV出力（Updated）\n",
    "df_keywords = pd.DataFrame([\n",
    "    {\n",
    "        \"country\": country,\n",
    "        \"regulation_targets\": \", \".join(regulation_targets[country]),\n",
    "        \"promotion_targets\": \", \".join(promotion_targets[country]),\n",
    "        \"unique_terms\": \", \".join(unique_terms[country])\n",
    "    }\n",
    "    for country in texts\n",
    "])\n",
    "\n",
    "# DataFrame 確認\n",
    "display(df_keywords)\n",
    "\n",
    "# CSV 保存\n",
    "df_keywords.to_csv(\"ai_policy_keywords2.csv\", index=False)\n",
    "\n",
    "# UTF-8 (BOM付き) で書き出す\n",
    "df_keywords.to_csv(\n",
    "    \"ai_policy_keywords3.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    header=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ff9c17a-3aab-44e3-9785-7a07b248c86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>regulation_targets</th>\n",
       "      <th>promotion_targets</th>\n",
       "      <th>unique_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>AI innovation,     AI adoption,     AI-related...</td>\n",
       "      <td>AI innovation,     Open-Source and Open-Weight...</td>\n",
       "      <td>frontier,     transformative,     unencumbered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>cybersecurity,     AI ethics,     data-related...</td>\n",
       "      <td>education reform,     human resources developm...</td>\n",
       "      <td>resilience,     sustainability,     diversity,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                 regulation_targets  \\\n",
       "0      US  AI innovation,     AI adoption,     AI-related...   \n",
       "1   Japan  cybersecurity,     AI ethics,     data-related...   \n",
       "\n",
       "                                   promotion_targets  \\\n",
       "0  AI innovation,     Open-Source and Open-Weight...   \n",
       "1  education reform,     human resources developm...   \n",
       "\n",
       "                                        unique_terms  \n",
       "0  frontier,     transformative,     unencumbered...  \n",
       "1  resilience,     sustainability,     diversity,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: CSV出力（Updated）\n",
    "def clean_terms(terms: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    各キーワードから改行コードと余分なダブルクオートを除去し、\n",
    "    前後の空白をトリムするヘルパー関数\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for t in terms:\n",
    "        s = t.replace(\"\\n\", \" \").replace('\"', \"\").strip()\n",
    "        cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "# Step 6: CSV出力（改良版）\n",
    "rows = []\n",
    "for country in texts:\n",
    "    regs   = clean_terms(regulation_targets[country])\n",
    "    promos = clean_terms(promotion_targets[country])\n",
    "    uniques= clean_terms(unique_terms[country])\n",
    "    rows.append({\n",
    "        \"country\": country,\n",
    "        \"regulation_targets\": \", \".join(regs),\n",
    "        \"promotion_targets\": \", \".join(promos),\n",
    "        \"unique_terms\": \", \".join(uniques)\n",
    "    })\n",
    "\n",
    "df_keywords = pd.DataFrame(rows)\n",
    "\n",
    "# DataFrame 確認\n",
    "display(df_keywords)\n",
    "\n",
    "# UTF-8 BOM付きで書き出し（Excel 互換性向上）\n",
    "df_keywords.to_csv(\n",
    "    \"ai_policy_keywords_clean.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    header=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e413c-96d8-4644-86aa-f886dda51a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f3047-a173-4b0b-b897-fea936b78138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: 結果統合＆CSV出力\n",
    "texts = load_pdfs(pdf_paths)\n",
    "text_chunks = chunk_texts(texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
